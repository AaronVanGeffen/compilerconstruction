\chapter{Static Analysis}

In this chapter we will discuss how we approached the static analysis in our compiler.
We started out doing Type Checking, but later implemented the more advanced Type Inference.
We will then show the inference rules and reflect on our implementation.

\section{Type Checking}

Type checking is a fairly simple analysis of a program.
By restricting the analysis to only verifying each specified type matches with what is expected, it is usually possible to treat each line by itself.
This does require a programmer to specify the types for each function or variable.
For some specific constructions, such as the empty list expression (\spl{[]}), this does require some special care: the type of the empty list is not as exactly defined as other constructions.

We implemented type checking by folding a function over the parsed syntax tree.
We track the state in a monad.
The functions recursively handle the nested segments of the declarations, functions, expressions and statements.
They either fail or return the result of the derivation of that segment.
Definitions of functions or variables are kept track of in the state with their derived type.
Encountered identifiers are looked up.

The data structures we used can be found in Listing~\ref{lst:typecheckdatastructures}.
We used several simple utility functions like \haskellinline{disallowVoid} to restrict the results of recursive \haskellinline{typeCheck} calls.
Listing~\ref{lst:typecheckunaryoperations} shows how this is applied for unary operations.

\begin{listing}[hbtp]
    \centering
    \inputhaskell{examples/typecheck_datastructures.hs}
    \caption{Data structures for type checking}\label{lst:typecheckdatastructures}
\end{listing}

\begin{listing}[hbtp]
    \centering
    \inputhaskell{examples/typecheck_unaryops.hs}
    \caption{Type checking unary operations}\label{lst:typecheckunaryoperations}
\end{listing}

We almost fully implemented type checking.
Because we moved on to do type inference, most of this code did not end up in the final compiler.

\section{Type Inference}
% \todo[author=Thom,inline]{Need we mention more? Show code?}

While type analysis is easy, it requires the programmer to specify many things in great detail while she is writing a program.
This results in lot of code that should be completely obvious.
It is not necessary to specify in \spl{Int bla = 3;} that \spl{bla} is of type \spl{Int}: this can be easily inferred from the literal \spl{3}.
Requiring to strictly specify types also may lead to having to write several copies of functions.
When writing a function like \spl/id (a) { return a; }/ it would be convenient to have it work for all types, but in languages that only do type checking it is usually not possible to write such a function down.

The general approach of type inference is to instead of deciding at every point if something is decidable, to generate constraints that the operations and functions should meet.
These constraints can thus be more general than concrete types.
By using information found in later sections of the program, these constraints can then be refined.

We implemented Hindley-Milner type inference following the approach laid out in~\cite{writeyouahaskelltypeinf}.
He implements type inference for an untyped lambda calculus, a language that is quite a lot simpler than SPL\@.
But as he writes what he is doing fairly clearly and chooses his data structures well, we were able to extend this to SPL quite easily.

Hindley-Milner type inference works by applying a set of type rules to the lines of code.
When something is not completely known, this is resolved by inserting a \emph{type variable}.
These variables will then later be \emph{unified} with (concrete) type information to get \emph{substitutions} that can be mapped over the environment again.
By doing this, we apply the knowledge we gain in later bits of code to essentially check the previous bits of code, as substitutions only exist if two types are unifiable.
We will discuss the type rules for SPL in Section~\ref{sec:typerules}.

\section{Type Rules}\label{sec:typerules}

\subsection{Type annotations}
First we need to show how to derive types from the literal type annotations.
We are going to need these for a lot of things, like declarations and function definitions.
\[
\infer{\Gamma \vdash a : \sigma}{(a, \sigma) \in \{(\mbox{`Bool'}, Bool), (\mbox{`Int'}, Int), (\mbox{`Char'}, Char)\} }
\]
We also need rules for lists and tuples.
These can also be used for their expressions.
\[
\infer{\Gamma \vdash [a] : [\sigma]}{\Gamma \vdash a : \sigma}\qquad
\infer{\Gamma \vdash (a, b) : (\sigma, \tau)}{\Gamma \vdash a : \sigma & \Gamma \vdash b : \tau}
\]

\subsection{Expressions}
Expressions show up in many places in the SPL grammar.
We can derive their rules as follows:
\begin{align*}
    \infer{\Gamma \vdash i : Int}{i \in \mathbb{Z}} &\qquad \mbox{Integer literals} \\
    \infer{\Gamma \vdash c : Char}{c \in \mathsf{Unicode}} &\qquad \mbox{Character literals} \\
    \infer{\Gamma \vdash b : Bool}{b \in \{\mbox{`True', `False'}\}} &\qquad \mbox{Boolean literals} \\
\end{align*}
\begin{align*}
    \infer{\Gamma \vdash\; !b : Bool}{\Gamma \vdash b : Bool} &\qquad \mbox{Unary inversion}\\
    \infer{\Gamma \vdash -i : Int}{\Gamma \vdash i : Int} &\qquad \mbox{Unary negation}\\
    \infer{\Gamma \vdash a\; o\; b : \sigma}{\Gamma \vdash a : \sigma & \Gamma \vdash b : \sigma & \sigma \in \{Char, Int\} & o \in \{+, -\}} &\qquad \mbox{Operations on Ints and Chars}\\
    \infer{\Gamma \vdash a\; o\; b : Int}{\Gamma \vdash a : Int & \Gamma \vdash b : Int & o \in \{*, /, \% \}} &\qquad \mbox{Operations on Ints}\\
    \infer{\Gamma \vdash a\; o\; b : Bool}{\Gamma \vdash a : \sigma & \Gamma \vdash b : \sigma & o \in \{==, !=, <, <=, >=, >\}} &\qquad \mbox{Comparison operations}\\
    \infer{\Gamma \vdash a\mathsf{.hd} : \sigma}{\Gamma \vdash a : [\sigma]} &\qquad \mbox{List head}\\
    \infer{\Gamma \vdash a\mathsf{.tl} : [\sigma]}{\Gamma \vdash a : [\sigma]} &\qquad \mbox{List tail}\\
    \infer{\Gamma \vdash a\mathsf{.fst} : \sigma}{\Gamma \vdash a : (\sigma, \tau)} &\qquad \mbox{Tuple first}\\
    \infer{\Gamma \vdash a\mathsf{.snd} : \tau}{\Gamma \vdash a : (\sigma, \tau)} &\qquad \mbox{Tuple second} \\
    \infer{\Gamma \vdash [] : [\sigma]}{} &\qquad \mbox{Empty List expression}
\end{align*}


\subsection{Declarations and functions}\label{sec:declarations}

Declarations are important as they allow to put new things into the state.
We need to consider following statements and declarations to propagate this definition.
Here we also see that an ordering of types arises: more general types can be substituted for more concrete types.
\[
    \infer{\Gamma \vdash a \; identifier = expr;next}{
        \Gamma \vdash type : \sigma &
        \Gamma \vdash expr : \tau &
        \sigma \sqsubseteq \tau &
        \Gamma,\; identifier : \sigma \vdash next
    }
\]
We need a separate version for declarations with $\mathsf{var}$ as they follow the expression's type instead of the specified type.
\[
    \infer{\Gamma \vdash \mathsf{var} \; identifier = expr;next : \tau}{
        \Gamma \vdash expr : \sigma &
        \Gamma,\; identifier : \sigma \vdash next
    }
\]

Function declarations follow this scheme. For convenience we will consider under specified functions as functions that have been specified with type variables instead. We will also only consider one argument, but this rule can be extended to no or multiple arguments without loss of generality. We insert the resultant type of this function in the environment when checking the body, to allow asserting return statements.
We also see the first appearance of the $\forall$ quantifier.
Using it allows for multiple uses of variable typed functions.
\[
    \infer{\Gamma \vdash identifier(arg)\; ::\; argtype\; \to\; rettype\; \{\; body\; \}\; next : \phi}{
        \begin{array}{r}
            \Gamma \vdash arg : \sigma \\
            \Gamma \vdash argtype : \tau\\
            \sigma \sqsubseteq \tau \\
            \Gamma \vdash rettype : \rho \\
            \Gamma, \mathsf{returntype} : \rho, identifier :\forall \tau,\;\rho. \tau \to \rho\vdash body : \alpha \\
            \Gamma, identifier :\forall \tau,\;\rho. \tau \to \rho \vdash next : \phi
        \end{array}
    }
\]
\textbf{Scoping} is resolved by deriving the next functions with the same state as the state before we declared this function.

\subsection{Statements}

Statements are the remaining meat of the schemes that SPL requires.
We start out by inferring \spl{if}-statements.
We will consider \spl{if}-statements that have not been specified with an \spl{else} clause as been having specified with an empty \spl{else} clause.
\[
    \infer{\Gamma \vdash \mathsf{if}\; (\; expr\;)\; \{\; thenStmts\; \}\;
    \mathsf{else}\; \{\; elseStmts\; \} : \sigma}{
            \Gamma \vdash expr : Bool &
            \Gamma \vdash thenStmts : \tau &
            \Gamma \vdash elseStmts : \rho
    }
\]
\spl{while}-statements follow analogously:
\[
    \infer{\Gamma \vdash \mathsf{while}\; (\; expr\;)\; \{\; stmts\; \}: \sigma}{
            \Gamma \vdash expr : Bool &
            \Gamma \vdash stmts : \tau
    }
\]
Update statements need to retrieve the identifier's type (with optionally specified fields) and check the type of the expression.
\[
    \infer{\Gamma \vdash identifier = expr : \rho}{
        \Gamma \vdash identifier : \sigma &
        \Gamma \vdash expr : \tau &
        \sigma \sqsubseteq \tau
    }
\]
Function call statements work similarly.
Like above, we will stick to single-argument functions.
This rule can also be used to handle function calls in expressions.
\[
    \infer{\Gamma \vdash identifier(arg) : \rho}{
        \Gamma \vdash arg : \sigma &
        \Gamma \vdash identifier : \tau \to \rho &
        \sigma \sqsubseteq \tau
    }
\]

Return statements are slightly more interesting.
We need to unify the type of the expression with the return type that was specified for the function we are currently evaluating.
This is why we extended the environment with that information in Section~\ref{sec:declarations}.
We will not consider empty return statements separately, but it can easily be imagined how they would work.
\[
    \infer{\Gamma \vdash \mathsf{return}\; expr; : \rho}{
        \Gamma \vdash expr : \sigma &
        \Gamma \vdash \mathsf{returntype} : \tau &
        \sigma \sqsubseteq \tau
    }
\]

\subsection{Plumbing}

Composition is needed to allow to put two statements together.
Because we resolve continuation in declarations separately, we do not need to handle state changes and can resolve expressions in parallel.
In practice, the collected unifiers need to be applied, so they are processed in sequence.
\[
    \infer{\Gamma \vdash e_1;\;e_2 : \rho}{
        \Gamma \vdash e_1 : \sigma &
        \Gamma \vdash e_2 : \tau
    }
\]

For the continuation of declarations and functions to work, we need an empty statement:
\[
    \infer{\Gamma \vdash \lambda}{}
\]

Finally, we need generalisation:
\[
    \infer{\Gamma \vdash e : \forall a.\; a}{
        \Gamma \vdash e : \sigma &
        e \not\in \operatorname{freevars}(\Gamma)
    }
\]


\subsection{Instantiation}

We already incorporated instantiation in the rules described above where relevant.

While implementing these typing rules we however made a slight mistake.
In general, they should not allow code like \spl{placeholder bla = 37;}, where the specified type is more general than the derived type of the expression.
Our unifier however treats these expressions like \spl{var bar = 42;} and derives that the type variable that \spl{placeholder} represents must be a placeholder for \spl{Int}.

\section{Reflection}

Doing type checking or type inference in Haskell works pretty well.
Traversing data structures is something functional languages excel at and handling the state in monads is greatly simplified and clarified by Haskell's \haskellinline{do}-notation.
As Listing~\ref{lst:typecheckunaryoperations} shows, it makes the recursion and result handling extremely straightforward.

But we definitely would do some things different if we were to try this again.
Because our AST does not contain any information on the location of pieces of syntax, error messages become rather meaningless and hard to debug.
Additionally, because our type inference and the checking of generated constraints are all immediate and very local, it is difficult to generate more than one error message.
We also have a hard time allowing mutual recursion or usage of functions before we process their definition, because of the way we walk the syntax tree.

Starting out with type checking helped gain some understanding of how to walk the syntax tree, but in the end we suspect that it cost us much more time than it saved us when we continued with type inference.
Most of the code we had for type checking we threw aside when we started on type inference.
While the approach was similar in some ways, it was different enough to prevent any significant reuse.

Because our approach is similar to a fold over the data structure with a single result, we throw out any type information that we gain along the way.
We later noticed that this put us in a rather unfortunate position in the code generation phase, as we did not have the information needed to decide overloaded function calls.
If we had kept the type information, for instance transforming SPL to LLVM IR would have been similar to writing a pretty printer.


% vim: set ts=4 sw=4 tw=0 et wrap :
